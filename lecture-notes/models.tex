\section{Preliminaries}

Before diving into the main topics today, we review typical strategies to analyze protocols involving hash functions.
Often we idealize hash functions as truly random functions.
Moreover, we provide only \emph{oracle} access to such functions;
that is, we assume that a prover learns about its hash function only by querying a given oracle.
Under these assumptions, the prover's complexity is lower bounded by its query complexity to its oracle.
This is called the classical \emph{random oracle model}.

Before we move to the quantum setting, we give a brief overview on how the proof of work algorithm can be analyze using this model.
To bound a cheating prover's success probability,
we keep a \emph{database} of its oracle queries and the responses.
Observe that a prover cannot win unless the database contains a response $y_i=0^ns$ that starts with $n$ zeroes,
up to a negligible error of him guessing randomly and getting lucky.
In order to receive a response that starts with $n$ zeroes, it takes $2^n$ queries on average,
which we conclude is a lower bound for the prover's time complexity.

Now let us consider the quantum setting.
When a prover is quantum, we need to modify our random oracle model accordingly.
The real world prover has access to the description of the hash function,
so it is able to evaluate its hash function over superposition inputs.
To model this behavior, we now give quantum access to the oracle by allowing superposition queries.
Mathematically, our oracle acts as a unitary map $\ket{x}\ket{y}\mapsto\ket{x}\ket{y+H(x)}$, as usual in the quantum setting.
We call this the \emph{quantum random oracle model}.

A recent work \cite{compressed-oracles} suggests a way to keep track of a cheating prover's quantum queries,
analogously to the classical database discussed earlier.
Without going into too much details,
we now provide a brief example as to how the database evolves.
The database begins as an empty list
$$D_0=\ket{[]}.$$
Suppose the cheating prover's first query is
$$\ket{x_1}\ket{0}+\ket{x_2}\ket{0},$$
and the result is
$$\ket{x_1}\ket{y_1}+\ket{x_2}\ket{y_2}.$$
The database is then updated to
$$D_1=\sum_{y_1}\ket{[(x_1, y_1)]} + \sum_{y_2}\ket{[(x_2, y_2)]},$$
which is, roughly speaking, a superposition of two lists,
each containing one element $(x_i, y_i)$.
Of course, this example is not fully general,
as the prover can use different amplitudes or construct a superposition of more than two terms.
Moreover, the database is \emph{purified},
which is another detail that we brush under the rug for the purpose of this discussion.

Now suppose the prover's next query is 
$$\ket{x_3}\ket{0}+\ket{x_4}\ket{0},$$
and the response is
$$\ket{x_3}\ket{y_3}+\ket{x_4}\ket{y_4}.$$
The resulting database is then
\begin{align*}
	D_2&=
	\sum_{y_1, y_3}\ket{[(x_1, y_1), (x_3, y_3)]}
	+ \sum_{y_1, y_4}\ket{[(x_1, y_1), (x_4, y_4)]}\\
	&+ \sum_{y_2, y_3}\ket{[(x_2, y_2), (x_3, y_3)]}
	+ \sum_{y_2, y_4}\ket{[(x_2, y_2), (x_4, y_4)]}.
\end{align*}
Observe that it is now a superposition of four lists.
The first two lists comes from the first term of $D_1$,
and the next two lists results from the second term of $D_2$.

One important observation is that each of the lists has lengths equal to the number of oracle queries,
even though the database might have exponentially many terms.
We will not directly use this fact today,
but this construction allows certain analysis strategies involving ``simulation" of the database.
